---
title: "ECO395M Homework 2"
author: "Kimberly Hu, Meilin Li, Yueting Zhang"
date: "2024-02-28"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      out.width = "75%", out.height = "75%")
options(width = 75)
```

```{r packages}
library(tidyverse)
library(ggplot2)
library(modelr)
library(rsample)
library(mosaic)
library(gamlr)
library(glmnet)
library(lubridate)
library(pROC)
library(foreach)
library(caret)
library(nnet)

```

# ECO395M Homework 2
### Kimberly Hu, Meilin Li, Yueting Zhang
### 2/8/2024


## 1. Saratoga house prices

This report presents the methodologies and findings from our analysis aimed at predicting property prices within the jurisdiction, to assist the local taxing authority in forming accurate market valuations for taxation purposes. Utilizing the SaratogaHouses data set as a basis for our models, we constructed a Lasso linear regression model and a K-nearest-neighbor regression model. Our objective was to identify the most accurate and reliable model for property valuation by comparing the out-of-sample performance of each model. 

### Baseline model

We started with a baseline linear model including 11 main effects as predictors of price. After training models and making predictions repeatedly for 100 times, we obtained an estimate of the average out-of-sample RMSE. RMSE measures the average magnitude of prediction errors and reflects deviations from the actual value. This value serves as a baseline for performance measurement of our models.  

Estimate of out-of-sample RMSE for baseline model:

```{r q1-1}

data(SaratogaHouses)

# baseline medium model with 11 main effects
# calculate the average rmse over 100 train-test splits

out_lm = do(100)*{
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  saratoga_lm = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
		fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)
  rmse_lm = rmse(saratoga_lm, saratoga_test)
  c(rmse_lm)
}

colMeans(out_lm)

```

### Lasso model

A Lasso regression model was utilized improve the prediction of property prices. In deciding which variables to include in the Lasso model, we observed that models with quadratic terms generally outperformed those with interaction terms. Upon examining the data set, we chose variables with significant numerical relevance, including lotSize, age, landValue, livingArea, and pctCollege, as our variables with quadratic terms. We also included all the main effects in our model. The model was fitted 100 times to ensure robustness, with the average out-of-sample RMSE from all iterations serving as a measure of the model's overall performance. The result indicates that the Lasso model outperforms the baseline model by achieving lower average RMSE, in other words, making lower prediction errors.

Estimate of out-of-sample RMSE for the Lasso model:

```{r q1-2}

# better linear model using Lasso
# include some quadratic terms

out_lasso = do(100)*{
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  
  saratoga_X_train = model.matrix(price ~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data = saratoga_train)
  saratoga_y_train = saratoga_train$price
  saratoga_lasso = glmnet(saratoga_X_train, saratoga_y_train, alpha = 1)

  saratoga_X_test = model.matrix(price ~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data = saratoga_test)
  saratoga_y_test = saratoga_test$price
  
  saratoga_lasso_predicted = predict(saratoga_lasso, newx = saratoga_X_test)
  saratoga_lasso_residuals = saratoga_y_test - saratoga_lasso_predicted
  
  rmse_lasso = sqrt(mean(saratoga_lasso_residuals^2))
  c(rmse_lasso)
}

colMeans(out_lasso)

```

### KNN model

Finally, we used KNN regression to predict house prices. For model construction, we selected the same variables as in the Lasso regression. The KNN model was trained on scaled data with cross-validation. Again, model performance is measured by average out-of-sample RMSE. It was found that the KNN model generates lower RMSE than both the baseline model and the Lasso model. 

Estimate of out-of-sample RMSE for KNN model:

```{r q1-3}

# KNN model
# use all variables and some quadratics

saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

Xtrain = model.matrix(~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data=saratoga_train)
Xtest = model.matrix(~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data=saratoga_test)

ytrain = saratoga_train$price
ytest = saratoga_test$price
  
scale_train = apply(Xtrain, 2, sd)  # calculate std dev for each column
Xtilde_train = scale(Xtrain, scale = scale_train)
Xtilde_test = scale(Xtest, scale = scale_train)

ctrl = trainControl(method="repeatedcv", number = 5, repeats = 3)
knnfit = train(Xtilde_train,
                   ytrain,
                   method = "knn",
                   trControl = ctrl,
                   tunelength = 100)
y_predict = predict(knnfit, Xtilde_test)
knn_errors = c(RMSE(ytest, y_predict))

print(knn_errors)

```

### Conclusion

In our comparative analysis of predictive models, the K-Nearest Neighbors (KNN) model emerged as the most accurate, achieving the lowest out-of-sample RMSE and evidently outperforming both the Lasso and the baseline linear models. This indicates that KNN, with its adaptability to data's intricacies, is the most reliable method for predicting property prices, making it the recommended choice for accurately determining market values of houses.


## 2. Classification and retrospective sampling

We are dealing with a data set from a German bank including information about loans. Our goal is to investigate how default probability is related to characteristics of loans, and try to make predictions based on the information.

```{r q2-barplot, echo=FALSE}

credit = read.csv("data/german_credit.csv", row.names = 1) 

default_rate = credit %>%
  group_by(history) %>%
  summarise(DefaultRate = mean(Default == 1))

ggplot(default_rate, aes(x = history, y = DefaultRate)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Default Rate by Credit History", 
       x = "Loan history", 
       y = "Default rate") +
  theme(plot.title = element_text(hjust = 0.5))

```

The bar plot above describes the default rate categorized by different levels of credit history. There are three levels of history: "Good", "Poor", and "Terrible". By observing the plot we can see that among three levels of credit history, the loans with "Good" history have the highest defaulted rate, while the loans with "Terrible" history have the lowest defaulted rate. This result is counter-intuitive, because it suggests that better credit history is related to higher loan default rate. 

We built a prediction model with logistic regression to see if some loan characteristics are good predictors of the default rate. The predictors used are: `duration`, `amount`, `installment`, `age`, `history`, `purpose`, `foreign`.

Coefficients reported by logistic regression model:

```{r q2-regression, echo=FALSE}

credit$installment <- factor(credit$installment)

set.seed(123)
credit_split = initial_split(credit, 0.8)
credit_train = training(credit_split)
credit_test = testing(credit_split)

logit_model <- glm(Default ~ duration + amount + installment + age + history + purpose + foreign, data = credit_train, family = binomial)

coef(logit_model) %>% round(2)

```

Confusion matrix:

```{r q2-confusion}

phat_test = predict(logit_model, newdata = credit_test) 
yhat_test = ifelse(phat_test > 0.5, 1, 0)
confusion_out_logit = table(y = credit_test$Default, yhat = yhat_test)
confusion_out_logit 

```

According to the confusion matrix: error rate = (4+60)/200=0.32, which indicates 68% accuracy. This is not a very high accuracy rate. 

Although the coefficients indicate reasonable relationship between default rate and some characteristics, we can still see counter-intuitive relationships, as well as an unsatisfactory accuracy rate. Combining the regression result and the bar plot, we can reasonably make a hypothesis that there's something in the data which prevents us from making successful predictions. 

Calculate counts of samples falling into different categories:

```{r count, echo=FALSE}

num_good <- sum(credit$history == "good")
print(paste("Number of 'good' credit history = ", num_good))
num_poor <- sum(credit$history == "poor")
print(paste("Number of 'poor' credit history = ", num_poor))
num_terrible <- sum(credit$history == "terrible") 
print(paste("Number of 'terrible' credit history = ", num_terrible))

```

Here we can see a huge gap between counts. That is to say, oversampling of some certain categories in the data may potentially be the reason why counter-intuitive statistical results occur. The loans with "good" credit history are underrepresented in the data, and a large portion of them happen to be defaulted loans. This problem may have been caused by how the data was originally selected. Since the loans in the data set was manually selected based on whether the loans have similar situations as the defaulted loans, these loans in the data set cannot represent the real life distribution of borrowers. 

Thus, this data set is an inappropriate one for building a predictive model for defaults. To classify borrowers into "low" and "high" defaulted probability categories, the bank needs a data set which more accurately represents the real distribution and situations of the potential borrowers. For example, randomized sampling can be a good way to achieve this. 


## 3. Children and hotel reservations

```{r q3-1}

hotels_dev = read.csv('data/hotels_dev.csv')
hotels_val = read.csv('data/hotels_val.csv')

```

### Model building

We are interested in building a model that predicts whether a hotel booking will have children on it. To evaluate the performance of our model, we first built two baseline models.  
1. Logistic regression model with the following predictors: `market_segment`, `adults`, `customer_type`, `is_repeated_guest`
2. Logistic regression model that uses all the existing predictors except `arrival_date`

```{r q3-2}

set.seed(9)

hotels_dev_split = initial_split(hotels_dev, prop = 0.8)
hotels_dev_train = training(hotels_dev_split)
hotels_dev_test = testing(hotels_dev_split)

# baseline model 1
hotels_logit1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, data=hotels_dev_train, family=binomial)

# baseline model 2
hotels_logit2 = glm(children ~ . - arrival_date, data=hotels_dev_train, family=binomial)

# predictions

phat_hotels_logit1 = predict(hotels_logit1, hotels_dev_test, type='response')
yhat_hotels_logit1 = ifelse(phat_hotels_logit1 > 0.5, 1, 0)
confusion_out_hotels_logit1 = table(y = hotels_dev_test$children,
                                    yhat = yhat_hotels_logit1)

phat_hotels_logit2 = predict(hotels_logit2, hotels_dev_test, type='response')
yhat_hotels_logit2 = ifelse(phat_hotels_logit2 > 0.5, 1, 0)
confusion_out_hotels_logit2 = table(y = hotels_dev_test$children,
                                    yhat = yhat_hotels_logit2)

```

`arrival_date` is a character variable that is difficult to incorporate directly into the model, so we extracted the month from the date, which is a factor of 12 levels, and included `month` as an additional predictor. We used backward selection on all predictors to selection the best combination of features. As a result, `previous_cancellations` and `deposit_type` are excluded.  

```{r q3-3}

# extract month from date
hotels_dev_train$month = as.factor(substr(hotels_dev_train$arrival_date, 6, 7))
hotels_dev_test$month = as.factor(substr(hotels_dev_test$arrival_date, 6, 7))

# we built model 3 using backward selection
# hotel_all = glm(children ~ . - arrival_date, data=hotels_dev_train, family=binomial)
# hotels_logit3_backward = step(hotel_all, direction="backward")

hotels_logit3 = glm(children ~ . - arrival_date - previous_cancellations - deposit_type, 
    data=hotels_dev_train, 
    family=binomial)

```

We produced confusion matrices and calculated TPR, FPR and FDR for the three models. Compared to baseline model 2, our model yields a higher TPR, which means that it correctly identifies more bookings with children. However, it also has higher FPR and FDR. 

```{r q3-5}

# fix problem of unrepresented categories in predict
is.prone <- function(x) is.factor(x) | is.character(x)
id <- sapply(hotels_dev, is.prone)
hotels_logit3$xlevels <- Map(union, hotels_logit3$xlevels, lapply(hotels_dev[id], unique))


# prediction and confusion matrix

phat_hotels_logit3 = predict(hotels_logit3, hotels_dev_test, type='response')
yhat_hotels_logit3 = ifelse(phat_hotels_logit3 > 0.5, 1, 0)
confusion_out_hotels_logit3 = table(y = hotels_dev_test$children,
                                    yhat = yhat_hotels_logit3)

confusion_matrices <- list(
  "Model 1" = confusion_out_hotels_logit1,
  "Model 2" = confusion_out_hotels_logit2,
  "Model 3" = confusion_out_hotels_logit3
)

print_confusion_matrices <- function(confusion_list) {
  for (model_name in names(confusion_list)) {
    cat(model_name, "Confusion Matrix:\n")
    print(confusion_list[[model_name]])
    cat("\n")
  }
}

print_confusion_matrices(confusion_matrices)

```

```{r q3-6}

# evaluation

models = list(
  model1 = c(TP = 0, FP = 0, TN = confusion_out_hotels_logit1[1,1], FN = confusion_out_hotels_logit1[2,1]),
  model2 = c(TP = confusion_out_hotels_logit2[2,2], FP = confusion_out_hotels_logit2[1,2], TN = confusion_out_hotels_logit2[1,1], FN = confusion_out_hotels_logit2[2,1]),
  model3 = c(TP = confusion_out_hotels_logit3[2,2], FP = confusion_out_hotels_logit3[1,2], TN = confusion_out_hotels_logit3[1,1], FN = confusion_out_hotels_logit3[2,1])
)

metrics = lapply(models, function(x) {
  TPR = x['TP'] / (x['TP'] + x['FN'])
  FPR = x['FP'] / (x['FP'] + x['TN'])
  FDR = x['FP'] / (x['FP'] + x['TP'])
  result = c(TPR = TPR, FPR = FPR, FDR = FDR)
  names(result) = c("TPR", "FPR", "FDR")
  return(result)
})

metrics_df = do.call(rbind, metrics)
rownames(metrics_df) = paste("Model", 1:3)
metrics_df

```

### Model validation step 1

We validated our model using the validation data set. The ROC curve is shown below. The curve lies above the straight line, which means that the model makes better predictions than random guesses. 

```{r q3-7}

# predict using validation data

hotels_val$month = as.factor(substr(hotels_val$arrival_date, 6, 7))

phat_hotels_val = predict(hotels_logit3, hotels_val, type='response')
y_hotels_val = hotels_val$children

# ROC curve

roc_obj_hotel = roc(y_hotels_val, phat_hotels_val)
plot(roc_obj_hotel, main="ROC Curve")


```

### Model validation step 2

By performing predictions for 20 folds and summing up the predictions within each fold, we found that our model consistently under-predicts the number of booking with children. The plot below compares the actual values and predicted values. This is still a lot of room for improvement. 

```{r q3-8}

folds = 20

hotels_val = hotels_val %>%
  mutate(fold_id = sample(rep(1:folds, length.out = nrow(hotels_val))))

hotels_val_cv = foreach(fold = 1:folds, .combine='rbind') %do% {
  hotel_val_folds = filter(hotels_val, fold_id == fold)
  hotel_val_folds_phat = predict(hotels_logit3, hotel_val_folds, type = "response")
  hotel_val_folds_yhat = ifelse(hotel_val_folds_phat > 0.5, 1, 0)
  c(sum_y=sum(hotel_val_folds$children), sum_yhat=sum(hotel_val_folds_yhat))
} %>% as.data.frame()

hotels_val_cv$fold = 1:nrow(hotels_val_cv)

results_long = pivot_longer(hotels_val_cv, cols = c(sum_y, sum_yhat), names_to = "Type", values_to = "Value")
results_long$fold = as.factor(results_long$fold)

ggplot(results_long, aes(x = fold, y = Value, color = Type, group = Type)) +
  geom_line() +
  geom_point() +
  labs(title = "Actual vs. Predicted Bookings with Children",
       x = "Fold", y = "Number of bookings with children") +
  scale_color_manual(values = c("sum_y" = "blue", "sum_yhat" = "red"), 
                     labels = c("Actual", "Predicted"))

```


## 4. Mushroom classification

For this question, our goal is to predict whether a mushroom is poisonous based on attributes of the mushroom. Since all of the variables are categorical, it makes sense to apply one-hot encoding for all of them, and use lasso-penalized logistic regression to select the variables that have predictive power. We dropped the `veil.type` variable, because it only has one value in this data set. 

A plot of our lasso model and the optimal lambda are shown below. 

```{r 4-1}

mushroom = read.csv('data/mushrooms.csv')

#mushroom %>%
#  select_if(~is.factor(.) | is.character(.)) %>%
#  map(~unique(.)) %>%
#  walk2(names(.), ~cat("Levels for", .y, ":", .x, "\n"))

# delete veil.type (only one level)
mushroom = subset(mushroom, select = -c(veil.type))

mushroom <- mushroom %>%
  mutate_if(is.character, as.factor)

set.seed(123)
mushroom_split = initial_split(mushroom, prop = 0.8)
mushroom_train = training(mushroom_split)
mushroom_test = testing(mushroom_split)

mushroom_x = model.matrix(class ~ . -1, data=mushroom_train)
mushroom_y = mushroom_train$class

mushroom_lasso = gamlr(mushroom_x, mushroom_y, family="binomial")
plot(mushroom_lasso)

# optimal lambda
optimal_lambda = log(mushroom_lasso$lambda[which.min(AICc(mushroom_lasso))])
optimal_lambda

```

Variables with non-zero coefficients selected by the algorithm:

```{r 4-2}

# coef
mushroom_beta = coef(mushroom_lasso) %>% 
  as.matrix() %>%
  as.data.frame() %>%
  filter(seg100 != 0)
selected = rownames(mushroom_beta)
selected

```

Using these variables, we were able to generate predicted probabilities of whether a mushroom is poisonous. The out-of-sample performance was evaluated using a ROC curve, which plots sensitivity (TPR) versus specificity (1 - FPR). 

```{r 4-3}

# prediction
mushroom_x_test = model.matrix(class ~ . -1, data=mushroom_test)
phat_mushroom = predict(mushroom_lasso, mushroom_x_test, type='response')

# ROC curve
roc_obj <- roc(mushroom_test$class, phat_mushroom)
plot(roc_obj, main="ROC Curve")

```

The optimal threshold for declaring a mushroom poisonous:

```{r 4-4}

optimal_coords <- coords(roc_obj, "best", ret="threshold")
optimal_threshold <- optimal_coords[1,1]
optimal_threshold

```

The confusion matrix below shows that almost all mushrooms are correctly classified using this threshold. TPR is over 99% and there is no false positive. 

```{r 4-5}

yhat_mushroom = ifelse(phat_mushroom > optimal_threshold, 1, 0)
confusion_mushroom = table(y = mushroom_test$class, 
                           yhat = yhat_mushroom)
confusion_mushroom

TPR <- confusion_mushroom["p","1"] / (confusion_mushroom["p","1"] + confusion_mushroom["p","0"])
FPR <- confusion_mushroom["e","1"] / (confusion_mushroom["e","1"] + confusion_mushroom["e","0"])
cat("True Positive Rate:", TPR, "\n")
cat("False Positive Rate:", FPR, "\n")

```

Although the model we built was extremely accurate with predicting poisonous mushrooms in this data set, we cannot assert that it is a good model for all cases. There could be training biases given the large number of categories in the predictors relative to the number of observations. In fact, based on our results, it is likely that the model suffers from overfitting. Further validations with new data sets are necessary to determine whether it is a good prediction model. 


