---
title: "HW2_question1"
output: pdf_document
date: "2024-02-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(glmnet)
library(tidyverse)
library(mosaic)
library(foreach)
library(modelr)
library(rsample)
library(caret)
```

# Q1 Saratoga house prices

## Property Valuation and Taxation Report

### Executive Summary

This report presents the methodologies and findings from our analysis aimed at predicting property prices within the jurisdiction, to assist the local taxing authority in forming accurate market valuations for taxation purposes. Utilizing the SaratogaHouses dataset as a basis for our models, we employed three main predictive strategies: baseline linear model, Lasso regression model and KNN model. Our objective was to identify the most accurate and reliable model for property valuation by comparing the performance of each model, to ensure fair and equitable taxation.

### Methodology

#### 1. Baseline model

We started with a baseline medium linear model including 11 main effects as predictors of SaratogaHouses dataset. After spliting the data into training and testing data, we used 100 different train-test splits to obtain an estimate of its average out-of-sample RMSE across these splits.

Estimate of out-of-sample RMSE for linear baseline model:

```{r q1-1}

data(SaratogaHouses)

# baseline medium model with 11 main effects
# calculate the average rmse over 100 train-test splits

out_lm = do(100)*{
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  saratoga_lm = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
		fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)
  rmse_lm = rmse(saratoga_lm, saratoga_test)
  c(rmse_lm)
}

colMeans(out_lm)

```

#### 2. Lasso model

To improve the prediction of property prices, a Lasso regression model was utilized. We divided the SaratogaHouses dataset into training and testing subsets. In deciding which variables to include in the Lasso model, we observed that variables with quadratic terms generally outperformed those with interaction terms. Upon examining the dataset, we chose variables with significant numerical relevance, including lotSize, age, landValue, livingArea, and pctCollege, as our quadratic terms. We then fitted a Lasso model to the training data. This process was repeated 100 times to ensure the model's robustness, with the average out-of-sample RMSE from all iterations serving as a measure of the model's overall performance.

Estimate of out-of-sample RMSE for the Lasso model:

```{r q1-2}

# better linear model using Lasso
# include some quadratic terms

out_lasso = do(100)*{
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  
  saratoga_X_train = model.matrix(price ~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data = saratoga_train)
  saratoga_y_train = saratoga_train$price
  saratoga_lasso = glmnet(saratoga_X_train, saratoga_y_train, alpha = 1)

  saratoga_X_test = model.matrix(price ~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data = saratoga_test)
  saratoga_y_test = saratoga_test$price
  
  saratoga_lasso_predicted = predict(saratoga_lasso, newx = saratoga_X_test)
  saratoga_lasso_residuals = saratoga_y_test - saratoga_lasso_predicted
  
  rmse_lasso = sqrt(mean(saratoga_lasso_residuals^2))
  c(rmse_lasso)
}

colMeans(out_lasso)


```

The result indicates that the Lasso model outperform the "medium" model by achieving lower out-of-sample RMSE.

#### 3. KNN model

Finally we used KNN to predict house prices. The dataset was divided into training and testing data. For model construction, we selected the same variables and quadratic terms that were used in the Lasso regression. The KNN model was trained on the scaled training data, employing 5-fold cross-validation, with 100 different numbers of neighbors (k values) being tested to determine the optimal k value. The model's predictive accuracy was assessed on the testing set using average out-of-sample RMSE. 

Estimate of out-of-sample RMSE for KNN model:

```{r q1-3}

# KNN model
# use all variables and some quadratics

saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

Xtrain = model.matrix(~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data=saratoga_train)
Xtest = model.matrix(~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data=saratoga_test)

ytrain = saratoga_train$price
ytest = saratoga_test$price
  
scale_train = apply(Xtrain, 2, sd)  # calculate std dev for each column
Xtilde_train = scale(Xtrain, scale = scale_train)
Xtilde_test = scale(Xtest, scale = scale_train)

ctrl = trainControl(method="repeatedcv", number = 5, repeats = 3)
knnfit = train(Xtilde_train,
                   ytrain,
                   method = "knn",
                   trControl = ctrl,
                   tunelength = 100)
y_predict = predict(knnfit, Xtilde_test)
knn_errors = c(RMSE(ytest, y_predict))

print(knn_errors)


```

### Conclusion

In our comparative analysis of three predictive models, the K-Nearest Neighbors (KNN) model emerged as the most accurate, achieving the lowest out-of-sample RMSE and evidently outperforming both the Lasso and baseline "medium" linear models. This indicates that KNN, with its adaptability to data's intricacies, is the most reliable method for predicting property prices, making it the recommended choice for accurately determining market values for taxation purposes.

