---
title: "HW2_question1"
output: pdf_document
date: "2024-02-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(glmnet)
library(tidyverse)
library(mosaic)
library(foreach)
library(modelr)
library(rsample)
library(caret)

```

## Linear model

We use the LASSO model with 10-fold cross-validation. Comparing the RMSE_out, we can tell that  the LASSO model (RMSE_out= 58016.81) clearly outperform the "medium" model(RMSE_out= 69233.8).

```{r}

data(SaratogaHouses)

# baseline medium model with 11 main effects
# calculate the average rmse over 100 train-test splits

out_lm = do(100)*{
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  saratoga_lm = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
		fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=saratoga_train)
  rmse_lm = rmse(saratoga_lm, saratoga_test)
  c(rmse_lm)
}

colMeans(out_lm)

```


```{r}

# better linear model using Lasso
# include some quadratic terms

out_lasso = do(100)*{
  saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  
  saratoga_X_train = model.matrix(price ~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data = saratoga_train)
  saratoga_y_train = saratoga_train$price
  saratoga_lasso = glmnet(saratoga_X_train, saratoga_y_train, alpha = 1)

  saratoga_X_test = model.matrix(price ~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data = saratoga_test)
  saratoga_y_test = saratoga_test$price
  
  saratoga_lasso_predicted = predict(saratoga_lasso, newx = saratoga_X_test)
  saratoga_lasso_residuals = saratoga_y_test - saratoga_lasso_predicted
  
  rmse_lasso = sqrt(mean(saratoga_lasso_residuals^2))
  c(rmse_lasso)
}

colMeans(out_lasso)


```

## KNN model

we choose the variables price + lotSize + age + sewer + waterfront + landValue + newConstruction(choose manually) normalize the data and use 5-fold cross validation to fit the KNN model. "tuneLength = 100" instructs the train function to consider 100 different values of K when selecting the best K value. RMSE of KNN clearly larger than RMSE of LASSO.
```{r KNN_model}

# KNN model
# use all variables and some quadratics

saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

Xtrain = model.matrix(~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data=saratoga_train)
Xtest = model.matrix(~ . -1 + lotSize^2 + age^2 + landValue^2 + livingArea^2 + pctCollege^2, data=saratoga_test)

ytrain = saratoga_train$price
ytest = saratoga_test$price
  
scale_train = apply(Xtrain, 2, sd)  # calculate std dev for each column
Xtilde_train = scale(Xtrain, scale = scale_train)
Xtilde_test = scale(Xtest, scale = scale_train)

ctrl = trainControl(method="repeatedcv", number = 5, repeats = 3)
knnfit = train(Xtilde_train,
                   ytrain,
                   method = "knn",
                   trControl = ctrl,
                   tunelength = 100)
y_predict = predict(knnfit, Xtilde_test)
knn_errors = c(RMSE(ytest, y_predict))

print(knn_errors)


```

